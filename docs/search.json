{
  "articles": [
    {
      "path": "about.html",
      "title": "Bayesian statistical world",
      "description": "Bayesian is the workplace for Gibbs sampler\n",
      "author": [],
      "contents": "\r\nGibbs Sampler involves working with conditional distributions - the core of Bayesian ideas. Let’s take a look at Bayesian World and how they are connected.\r\nBackground: Bayesian V.S. Frequentist\r\nFrequentist is the dominant philosophy and category in the modern statistical world. In fact, many concepts/ thinking/ logic we have been exposed from statistics are from Frequentist beliefs, such as “confidence interval”, “p-value”, “the true value of an unknown parameter”, etc. However, Gibb’s sampler is a method that applies conditional probability distribution, where the “conditioned” nature indicates its Bayesian flavor. This page serves as a refresher to get a taste of Bayesian Statistics - we will briefly talk about its knowledge building process of estimating unknown parameters, learn Bayes rule, and do a little exercise on deriving the posterior distribution of an unknown parameter.\r\nUnlike Frequentists who deem unknown parameters as unknown, fixed constants, Bayesians think about them as random variables and constantly changing their beliefs about unknown parameters based on the data we truly see - they start by proposing a possible probability distribution for the unknown parameter (prior distribution), and find out what they actually see from real data to update/ change the beliefs by deriving posterior distribution based on the prior distribution and data. Furthermore, by seeing more data, they continue to update their knowledge on the parameter (for example, shrink the parameter space/ support by lowering the plausibility of some values of which the parameter can take).\r\nWe can imagine the parameter is a Plasticine or modeling clay. When we buy a Plasticine from a store, it has its original shape (prior - the beginning hypothesis/ information we have about the parameter), and its shape constantly changes corresponding to the external forces applied (data we’ve seen so far). The resulted shapes are the posterior - the information we gained about parameter after seeing data.\r\nThe end goal of Bayesian estimation is not to see a convergence and shrinkage of parameter space and determine the most likely value of a parameter- that sounds like “Frequentist”. In Bayesian, we never end updating the knowledge based on data.\r\nSounds like there is lots of “based on”, “dependent on”, and “after seeing…” in Bayesian - YES, this explains why our topic of Gibbs Sampling is contextualized on Bayesian side - Gibbs sampler involves with working with conditional distributions.\r\nFigure below illustrates Bayesian knowledge-building process: Johnson, A. et.al. If we heard from many 5-star reviews from a restaurant that you are going to visit, we may start expecting it to be a 5-star restaurant.\r\nYou sit down, and they serve you a really soggy noodles, you’d be upset and surprised - at the same time, you overthrow your previous assumption about the restaurant and thought it was really just a 3-star level. Then, they serve you a really yummy dish and you change your impression back to 4-star as for now…\r\nAn simplification of Bayesians thinking processThrough observing and changing, Bayesians are giving themselves more flexibility of estimating parameters of interest. They do estimations based on “Bayes rule” - a central idea and foundation of Bayesian statistics.\r\nBayes Rule\r\nConnecting back to probability terms, let’s remind ourselves the Bayes rule of two events \\(A\\) and \\(B\\), generalized by equation below:\r\n\\[\r\nP(A| B )= \\frac{P(A\\cap B)} {P(B)}=\\frac{P(B|A)P(A)}{P(B)}\r\n\\]\r\nwhere:\r\n\\(P(A)\\) and \\(P(B)\\) are the independent probabilities that they happen\r\n\\(P(A\\cap B)\\) is the probability that both A and B happen\r\n\\(P(B|A)\\) is the conditional probability given A happens\r\nBy knowing the independent probabilities of \\(A\\) and \\(B\\), and seeing the probability of \\(B\\) happens given A happens, we can derive what do we know about \\(A\\) given B.\r\nIn Bayes, \\(A\\) will reflect our known parameters, and \\(B\\) will reflect our data. Let’s take a further step to see how this connects to probability distribution as a whole.\r\nUse Bayes rule in probability distribution\r\nSuppose we are interested in the unknown parameter \\(\\theta\\). How do we update our guess?\r\nFirst, We start by proposing a prior distribution to the unknown parameter \\(\\theta\\): denote this as \\(p(\\theta)\\) if discrete or \\(f(\\theta)\\) if continuous. For example, if \\(\\theta\\) represents a probability, then one may suggest their prior to be \\(\\theta \\sim Unif(0,1)\\). There is no right or wrong/ good or bad judgment calls to priors- they are completely subjective and based on personal knowledge/ background research to the parameter.\r\nNext, we will incorporate our data by calculating the likelihood of seeing a sequence of observations \\(\\vec{X}\\) under \\(\\theta\\): denoted as \\(f(\\vec{X}|\\theta)\\).\r\nThirdly, We are interested in posterior distribution of the parameter - update! Denote this as \\(g(\\theta|\\vec{X})\\) to differentiate the updated distribution.\r\nAccording to the Bayes rule, we can calculate the posterior distribution \\(g(\\theta|\\vec{X})\\) by (suppose \\(X, \\theta\\) are both continuous random variables):\r\n\\[\r\ng\\left(\\theta \\mid \\vec{X} \\right) =\\frac{f\\left(\\vec{X} \\mid \\theta\\right) f(\\theta)}{f(\\vec{X})}\r\n\\]\r\nwhere \\(f\\left(\\vec{X} \\mid \\theta\\right)\\) is the joint pdf/ likelihood given \\(\\theta\\) and the denominator \\(f(\\vec{X})\\) is the marginal pdf of \\(\\vec{X}\\).\r\nNotice that the posterior distribution of \\(\\theta\\) does not depend on \\(\\vec{X}\\), that leads us to a convention in deriving the posterior distribution \\(g(\\theta \\mid \\vec{X})\\) because we can ignore the actual composition of \\(f(\\vec{X})\\) as it only serves as the normalizing constant in deriving a pdf for a random variable. Therefore:\r\n\\[\r\ng\\left(\\theta \\mid \\vec{X} \\right) \\propto {f\\left(\\vec{X} \\mid \\theta\\right) f(\\theta)}\r\n\\]\r\nNote that the pdf of posterior distribution of \\(\\theta\\) is not equal to the right hand side - we still need the marginal pdf of \\(\\vec{X}\\) to calculate the exact pdf. But the above equation gives us the kernels in a pdf - they are sufficient enough to let us find out the posterior distribution of \\(\\theta\\).\r\nExercise: deriving posterior distribution for \\(\\theta\\)\r\nLet’s try a simple example (from Dr. Kelsey Grinde’s in class material on Bayesian Statistics). Suppose we observed one random observation of a coin flip, \\(X\\sim Bern(p)\\), and we are interested in \\(p\\), where we assume its prior distribution: \\(p \\sim Beta (a,b)\\). We derive the posterior distribution \\(g(p \\mid X)\\):\r\n\\[\r\n\\begin{aligned}\r\ng(p|X)  &\\propto f(X\\mid p) f(p)\\\\\r\n&= [p^X (1-p)^{1-X}] \\quad [\\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)} p^{a-1} (1-p)^{b-1}]\\\\\r\n& \\propto p^X(1-p)^{1-X} p^{a-1}(1-p)^{b-1} \\\\\r\n& =p^{X+a-1}(1-p)^{1-X+b-1} \\\\\r\n& p \\mid X \\sim \\operatorname{Beta}(X+a, 1-X+b) \\\\\r\n&\r\n\\end{aligned}\r\n\\]\r\nWe only conserve terms with \\(p\\) and dump anything else such as constants \\(a, b\\). Notice how \\(a,b, X\\) and the number of trial (1 in this case since we only have one random observation) are incorporated into the posterior distribution.\r\nThe example above works with single unknown parameter \\(p\\), and Gibbs sampling works with Bayesian to obtain posterior samples of multiple unknown parameters (many dimensions). Furthermore, it is noteworthy that Gibbs is only applied to situations when we have multiple unknown parameters and it is hard to calculate the joint, posterior distribution of all parameters using the method above (so we do not have a single posterior distribution, but many - this means we cannot derive i.i.d. samples but dependent samples), and it is easier/ feasible to derive their individual conditional posterior distributions. We will learn about it more on the next page!\r\n\r\n\r\n\r\n",
      "last_modified": "2023-04-29T16:00:00-05:00"
    },
    {
      "path": "gibbs.html",
      "title": "Gibbs Sampling",
      "description": "Some additional details about the website\n",
      "author": [],
      "contents": "\r\nBackground\r\nProbability Refresher\r\nGibbs sampling is used for multivariate distributions, or distributions that depend on more than one variable. Two types of multivariate distributions are joint and conditional.\r\nJoint Probability Distribution - probability based on two independent variables (ex. p(x, y))\r\nConditional Probability Distribution - variables are dependent on each other (ex. p(x) given Y = y)\r\nJoint vs Conditional DistributionsMarkov Chain Monte Carlo (MCMC)\r\nMarkov Chain Monte Carlo is used to estimate probabilities by simulating repetitions of an experiment. It is usually used in situations where an exact probability is difficult to calculate. [Larsen & Marx, 2018]\r\nMCMC: Metropolis Hastings\r\nMetropolis Hastings is a classic MCMC method used to obtain a sequence of random samples where the direct distribution is hard to find. In this method, a proposed distribution is utilized to sample new states. At each iteration, a new state is proposed. Gunderson, 2020\r\nMetropolis Hastings also utilizes an acceptance rate. When you get a random sample, a probability equation is used to decide whether or not to accept the new, proposed values. Gibbs Sampling is a special case of Metropolis Hastings with conditional distributions and an acceptance rate = 1. This means that in Gibbs Sampling, the new proposed state is accepted 100% of the time. Jingyi Jessica Li\r\nConcept\r\nGibbs Sampling is a Markov Chain Monte Carlo (MCMC) method that that is used for multidimensional models. Gibbs is used when sampling using joint distributions is too difficult, but it is easy to sample from conditional probabilities.\r\nGibbs Sampling Definition Suppose we want to obtain \\(n\\) samples of \\(X = (x_1, x_2, x_3, ..., x_m)\\) from a joint distribution \\(p(x,y)\\).\r\nIn Gibbs sampling, we will sample each \\(x^m\\) conditional on others, that is, in iteration \\((n+1)\\):\r\n\\[\r\n\\begin{aligned}\r\nx_{n+1}^{(1)}&\\sim P(x^{(1)}|x_n^{(2)},x_n^{(3)},...,x_n^{(m)}) \\\\\r\nx_{n+1}^{(2)}&\\sim P(x^{(2)}|x_{n+1}^{(1)},x_n^{(3)},...,x_n^{(m)}) \\\\\r\nx_{n+1}^{(m)}&\\sim P(x^{(m)}|x_{n+1}^{(1)},...,x_{n+1}^{(m-1)})\r\n\\end{aligned}\r\n\\]\r\nIn Practice\r\n1. Set \\((x_0,y_0)\\) to some starting value.\r\n2. Sample \\(x_1 \\sim p(x|y_0)\\). Alternatively, \\(X|Y = y_0\\). This produces \\((x_1, y_0)\\).\r\n3. Then, sample \\(y_1 \\sim p(y|x_1)\\) to arrive at the second point in the distribution \\((x_1,y_1)\\)\r\n4. Repeat steps 2 and 3, \\(n\\) times.\r\nThis produces a sequence of pairs of random variables, \\((X_0,Y_0),(X_1,Y_1),(X_2,Y_2),...\\), which satisfies the property of being a Markov chain. Note that the conditional distribution of \\((X_i,Y_i)\\), given all the previous pairs, only depends on \\((X_{i-1},Y_{i-1})\\) Steorts\r\nIn a Bayesian Perspective\r\nGibbs Sampling can be used to simulate a Markov chain distribution of unknown parameter \\(\\theta\\). Let’s call this distribution \\(\\pi (\\theta)\\).\r\nInitial conditions\r\nSuppose \\(\\theta\\) may be partitioned into \\(\\theta = (\\theta_1,...,\\theta_r)\\)\r\nIt is then possible to simulate a random value of \\(\\theta_i\\) from a full conditional distribution \\(\\pi(\\theta_i | \\theta_1, \\theta_2, ..., \\theta_{i-1},\\theta_{i+1},...,\\theta_r)\\) for \\(i = 1, 2, ...\\)\r\n\r\nUse initial conditions to simulate the distribution \\(\\pi (\\theta)\\)\r\nStart with initial values \\(\\theta^{(0)} = (\\theta_1^{(0)},...,\\theta_r^{(0)})\\)\r\nThen, use the following steps for \\(m = 1, 2, ...\\):\r\nStep 1: Sample \\(\\theta_1^{(m)}\\) from \\(\\pi(\\theta_1 | \\theta_2^{(m-1)}, \\theta_3^{(m-1)},..., \\theta_4^{(m-1)})\\)\r\nStep 2: Sample \\(\\theta_2^{(m)}\\) from \\(\\pi(\\theta_2 | \\theta_1^{(m)}, \\theta_3^{(m-1)},..., \\theta_d^{(m-1)})\\)\r\nStep r: Sample \\(\\theta_r^{(m)}\\) from \\(\\pi(\\theta_r | \\theta_1^{(m)}, \\theta_2^{(m)},..., \\theta_{r-1}^{(m)})\\)\r\n\r\nBy the Monte Carlo rules, it is easy to show that if \\(\\theta^{(m-1)}\\sim \\pi(\\theta)\\), then \\(\\theta^{(m)}\\sim \\pi(\\theta)\\), and thus is the distribution \\(\\pi (\\theta)\\). Pritchard et al., 2000\r\nProperties\r\nMixing Rate: refers to how quickly the sample averages converge\r\nIndicates converging to a reasonable probability distribution\r\nA faster mixing rate means that the algorithm “mixes well”\r\nBurn-In: property of Metropolis Hastings MCMC\r\nIt takes a few trials to converge to a reasonable probability distribution\r\nMust discard the first “B” samples that do not represent the data well\r\nFirst “B” samples are referred to as the burn-in period\r\nA good model will have a small burn in period\r\nLimitations\r\nSome potential limitations and situations where Gibbs sampling can’t be used would be in situations where there are areas of extremely high probability or of extremely low probability. By the nature of conditional probability distributions, if the current point lands in a high probability region, the next point won’t move. This is because it is solely dependent on the previous point, and the entire sample space will be limited to this very high probability peak.\r\nOn the other hand, in a low probability region, you won’t be able to find any higher probability regions either. This is because sampling fails when it comes across a region where probability = 0. If the probability of any data being at a certain value in a dimension is zero, then you won’t be able to sample a new point in that dimension because the entire sample space will also have a probability of zero.\r\nA simulation example: Exponential and gamma model\r\nSet up\r\nConsider an exponential model for our random observation\r\n\\(X_1,...,X_n \\stackrel{iid}{\\sim} \\text{Exponential}(ab)\\) (where the\r\nparameter \\(\\lambda\\) is the product of \\(a\\) and \\(b\\)):\r\n\\[p(X_i|a,b) = abe^{-abx} \\quad \\text{for} (x>0)\\] \\(a\\) and \\(b\\) is our\r\nunknown parameters. Now, suppose we put the following independent\r\npriors on \\(a\\) and \\(b\\): \\[a{\\sim} \\text{Gamma}(1,1)\\]\r\n\\[b{\\sim} \\text{Gamma}(1,1)\\]\r\nRecall from probability: If random variable A has Gamma(1,1)\r\ndistribution, it has the pdf: \\(f_A(a) = e^{-a}\\).\r\nTherefore, since \\(a\\) and \\(b\\) are independent priors, we can calculate\r\njoint distribution of our prior by simply multiplying their pdfs\r\ntogether:\r\n\\[p(a,b) = e^{-a-b} \\quad \\text{for} (a,b>0)\\]\r\nWhy is conditional distribution easier than joint?\r\nRemember: our unknown parameters/ interests are \\(a\\) and \\(b\\).\r\nTo start with, given the above, we really want to know what the joint\r\nposterior distribution of \\(p(a,b|X)\\) looks like. Unfortunately, however,\r\nwe can’t. In fact, Kristy and Regan are going to claim that, instead of\r\ndirectly sampling from the joint posterior distribution\r\n\\(p(a,b|\\vec{X})\\), it is easier to sample from conditional distributions:\r\n\\(p(a|\\vec{X},b)\\) and \\(p(b|\\vec{X},a)\\). (vector \\(X\\) just means we are\r\ninterested at a random sample - a bunch of \\(X_i\\)s.)\r\nWhy? Let’s write out their formulas:\r\nJoint (hard!):\r\n\\[\r\np(a,b|\\vec{X})=\\frac{p(a,b,\\vec{X})}{p(\\vec{X})}\r\n\\]\r\nConditional (easy!):\r\n\\[\r\np(a|b,\\vec{X}) = \\frac{p(a,b,\\vec{X})}{p(b,\\vec{X})}\r\n\\]\r\nNote that they’re both proportional to the whole, joint distribution\r\n\\(p(a,b,\\vec{X})\\). Let’s calculate this: \\[\r\n\\begin{aligned}\r\np(a,b,\\vec{X})&= p(\\vec{X}|a,b)*p(a,b)\\\\\r\n&=\\Pi_{i=1}^n p(X_i|a,b) * e^{-a-b}  \\\\\r\n&= \\Pi_{i=1}^n abe^{-abx_i} * e^{-a-b} \\\\\r\n&= (ab)^n e^{-ab\\sum_{i=1}^nx_i}* e^{-a}*e^{-b} \\\\\r\n&= (ab)^n e^{-a(b\\sum_{i=1}^nx_i+1)}* e^{-b}\r\n\\end{aligned}\r\n\\] Let’s start with the conditional distribution \\(p(a|b,\\vec{X})\\) that\r\nworks:\r\n\\[\r\n\\begin{aligned}\r\np(a|b,\\vec{X}) &\\propto p(a,b,\\vec{X})\\\\\r\n&=(ab)^n e^{-a(b\\sum_{i=1}^nx_i+1)}* e^{-b}\\\\\r\n&\\propto a^ne^{-a(b\\sum_{i=1}^nx_i+1)} \\quad \\quad\\text{dump b-related terms}\\\\\r\n\\end{aligned}\r\n\\]\r\nThis is a Gamma distribution!\r\n\\[\r\na|b,\\vec{X}\\sim \\text{Gamma}(n+1,b\\sum_{i=1}^nx_i +1)\r\n\\]\r\nSymmetrically, the conditional distribution of \\(b\\) is the same: \\[\r\nb|a,\\vec{X} \\sim \\text{Gamma}(n+1,a\\sum_{i=1}^nx_i +1)\r\n\\]\r\nWhat about our joint, posterior distribution \\(p(a,b|\\vec{X})\\)? \\[\r\n\\begin{aligned}\r\np(a,b|\\vec{X}) &\\propto p(a,b,\\vec{X})\\\\\r\n&=(ab)^n e^{-a(b\\sum_{i=1}^nx_i+1)}* e^{-b}\\\\\r\n\\end{aligned}\r\n\\] We can’t dump any terms, and this does not look familiar.\r\nGetting to Gibbs\r\nRelating back to Gibbs sampling, since we have two dimensions \\(a\\) and\r\nb$, our Gibbs samples are: \\((a_0, b_0)\\), \\((a_1, b_1)\\), \\((a_2, b_2)\\),\r\n…… \\((a_n, b_n)\\), where the \\(n^{th}\\) sample \\((a_n, b_n)\\), will always\r\ndepend on the previous one \\((a_{n-1}, b_{n-1})\\), forming a Markov Chain.\r\nWe draw samples as:\r\n1. Choose the initial sample \\((a_0, b_0)\\)\r\n2. Draw \\(a_1\\sim p(a|b_0,\\vec{X})\\), \\(b_1\\sim p(b|a_1,\\vec{X})\\) - this is\r\n\\((a_1, b_1)\\)\r\n3. Draw \\(a_2\\sim p(a|b_1,\\vec{X})\\), \\(b_2\\sim p(b|a_2,\\vec{X})\\)\r\nThis means, after seeing some data \\(\\vec{X}\\), where we said\r\n\\(X_i\\sim \\text{Exponential}(ab)\\), we will continuously draw posterior\r\nsamples, from two dimensions \\(a\\) and \\(b\\), based on the following\r\ndistribution (\\(N=\\) number of random observation of \\(X\\), \\(j=\\) times of\r\niteration):\r\n\\[\r\na_{j+1}|b_{j},\\vec{X}\\sim \\text{Gamma}(N+1,b_j\\sum_{i=1}^nx_i\r\n+1)\r\n\\]\r\n\\[\r\nb_{j+1}|a_{j+1},\\vec{X}\r\n\\sim \\text{Gamma}(N+1,a_{j+1}\\sum_{i=1}^nx_i +1)\r\n\\]\r\n(continued) R: Gibbs Sampler\r\nThe following content is adapted from Dr. Grinde’s in class activity on\r\nGibb’s sampling. Here are the conditional distributions if needed:\r\n\\[\r\na_{j+1}|b_{j},\\vec{X}\\sim \\text{Gamma}(N+1,b_j\\sum_{i=1}^Nx_i\r\n+1)\r\n\\]\r\n\\[\r\nb_{j+1}|a_{j+1},\\vec{X}\r\n\\sim \\text{Gamma}(N+1,a_{j+1}\\sum_{i=1}^Nx_i +1)\r\n\\]\r\nSet up\r\n\r\n\r\nset.seed(1)\r\n# true data distribution\r\nN <- 100  # 100 Xi observations\r\ntruea <- 3\r\ntrueb <- 4\r\nX <- rexp(n = N, truea*trueb) #create 100 observations where X~exp(ab)\r\n\r\nXsum = sum(X) # for later use\r\n\r\n# set up priors (mean)\r\na0 <- mean(rgamma(1000, shape = 1, rate = 1))\r\nb0 <- mean(rgamma(1000, shape = 1, rate = 1))\r\n\r\n\r\nWe are setting up our data distribution to be an exponential distribution with \\(\\lambda = ab\\). In our data, we are setting the true value of \\(a = 3\\) and the true value of \\(b = 4\\). Additionally, we are setting up our priors that we have previously calculated. Next, we need to choose starting values for our Gibbs sampler. It is important to choose a starting point that is reasonably close to our true values. Therefore, we decided to use the mean of our priors, \\(a_0\\) and \\(b_0\\), to be our starting values.\r\n\r\n\r\nset.seed(1)\r\n# store posterior samples of a and b\r\nalphas <- c()\r\nbetas <- c()\r\n\r\n# input a0, b0\r\n# we are choosing the starting points to be the mean of our prior distribution\r\nalphas[1] <- a0\r\nbetas[1] <- b0\r\n\r\n# choose how many iterations of Gibbs sampling to run\r\nj <- 1000\r\n\r\n\r\nNote that we now choose our average prior values as our initial sample \\((a_0, b_0)\\) as our initial sample. We are drawing 100 iid observations of \\(X_i\\)s and running our Gibbs Sampling for 1000 iterations (\\(j\\)).\r\nOur sampler will start at our initial values \\(a_0,b_0\\). Then, it will update through 1000 iterations. Since we already know that \\((a=3, b=4)\\) are the “correct” values for \\((a,b)\\), we can predict that our Gibbs sampling will fluctuate around these values.\r\nRun!\r\n\r\n\r\nset.seed(1)\r\n# run for j (1000) iterations\r\n\r\nfor(i in 2:j){\r\n# update a and b\r\na <- rgamma(n = 1, shape = N+1, rate = betas[i-1]*Xsum + 1)\r\nalphas[i] <- a\r\n\r\nb <- rgamma(n = 1, shape = N+1, rate = alphas[i]*Xsum + 1)\r\nbetas[i] <- b\r\n}\r\n\r\n\r\nVisualizations\r\n\r\n\r\nset.seed(1)\r\n# Histogram of samples\r\npar(mfrow=c(1,2))\r\nhist(alphas, xlab = expression(alpha), main = '')\r\nhist(betas, xlab = expression(beta), main = '')\r\n\r\n\r\n\r\nThis looks correct because our true values of \\(a\\) and \\(b\\) are 3 and 4 respectively.\r\nTrace Plot: the behavior of the samples over j iterations:\r\n\r\n\r\nset.seed(1)\r\niterations <- 1:j\r\npar(mfrow=c(1,2))\r\nplot(alphas ~ iterations, xlab = 'Iteration', ylab = expression(alpha), type = 'l')\r\nabline(h = a0, col = \"red\")\r\ntext(x=850, y= a0 + 0.5, 'a0', col = \"red\")\r\nabline(h = truea, col = \"blue\")\r\ntext(x=850, y=truea-0.5, 'True a', col = \"blue\")\r\nplot(betas ~ iterations, xlab = 'Iteration', ylab = expression(beta), type = 'l')\r\nabline(h = b0, col = \"red\")\r\ntext(x=850, y=b0 + 0.5, 'b0', col = \"red\")\r\nabline(h = trueb, col = \"blue\")\r\ntext(x=850, y=trueb+0.5, 'True b', col = \"blue\")\r\n\r\n\r\n\r\nAnalyzing algorithm performance\r\nFrom observing the trace plot above, what can we say about our Gibbs Sampler’s performance? What factors do you think that are involved in\r\nshaping the trace plots?\r\nANSWERS:\r\nFrom observing our trace plots, we can say that our Gibbs Sampler is a good simulation to find our true values. We can see that even though it starts at our initial values, it quickly jumps to fluctuate around our true values. We could say that there is a burn in period of about 100 trials for both parameters because it takes about 100 trials to converge around the true values.\r\nMany factors can determine our trace plots. Observed data (“Xsum”), (the way of choosing) the starting values of both parameters, sample size (“\\(N\\)”), number of\r\niterations (“\\(j\\)”) - if we run for enough long time, the samples will\r\nconverge and starting value does not matter that much. If we choose\r\nwisely on starting value, it may take us less time to see that the\r\nvalues \\(a\\) and \\(b\\) converge (greater mixing rate: evidence of a good\r\nMCMC algorithm performance) and their trace plots become stable.\r\nNoting that fluctuations are not necessarily bad. Mixing rate, which is\r\nhow fast that sample averages coverage, determines MCMC performance.\r\nOur trace plots could be improved by changing the initial starting\r\nvalues, \\((a_0, b_0)\\), and factors mentioned above all come in to play in\r\ndetermining the convergence shape of the trace plots.\r\nExploring\r\nWe can also observe what our trace plots would look like under less ideal conditions.\r\nDifferent starting values\r\nFor example, what would happen if we choose a starting value that was very far away from our true \\(a\\) and \\(b\\)? In this trial, we set \\(a\\) and \\(b\\) to each start at 100.\r\nHigh Starting Point: a = 100, b = 100\r\nLet’s check the mean and median of these posterior samples, and compare them with our true values of \\(a=3\\) and \\(b=4\\)\r\n\r\n\r\n# the mean and median of posterior samples\r\nc(mean(alphas[20:1000]),median(alphas[20:1000]))\r\n\r\n[1] 3.234724 3.042965\r\n\r\nc(mean(betas[20:1000]),median(betas[20:1000]))\r\n\r\n[1] 3.979042 3.758907\r\n\r\nOn the other hand, what happens if we pick a pair of extremely small starting points?\r\nLow Starting Point: a = 0.0001, b = 0.0001\r\nLet’s check the posterior mean and median again:\r\n\r\n\r\n# the mean and median of posterior samples\r\nc(mean(alphas[100:1000]),median(alphas[100:1000]))\r\n\r\n[1] 3.153170 2.988194\r\n\r\nc(mean(betas[100:1000]),median(betas[100:1000]))\r\n\r\n[1] 4.052377 3.808682\r\n\r\nIn both extremes, our Gibbs Sampler has a good mixing rate and fluctuates around the true values for \\(a\\) and \\(b\\) after approximately 100 trials (this is by eyeballing - we should really look in depth at the cut off of burn in if we want to be more precise).\r\nDifferent prior distributions\r\nAnother place to explore would be if we used a different prior. In our model, we used a joint prior of \\(p(a,b) = e^{-a-b} \\quad \\text{for} (a,b>0)\\) with individual priors \\(a {\\sim} \\text{Gamma}(1,1)\\) and \\(b{\\sim} \\text{Gamma}(1,1)\\). Hypothetically, what if our priors had higher rates?\\[a{\\sim} \\text{Gamma}(1,5)\\]\r\n\\[b{\\sim} \\text{Gamma}(1,5)\\]\r\nThis would change our conditional distributions to be:\r\n\\[\r\na|b,\\vec{X}\\sim \\text{Gamma}(n+1,b\\sum_{i=1}^nx_i +5)\r\n\\]\r\n\\[\r\nb|a,\\vec{X}\\sim \\text{Gamma}(n+1,a\\sum_{i=1}^nx_i +5)\r\n\\]\r\nChanging Priors to Gamma(1,5)With a different prior distribution of \\(a\\) and \\(b\\), we can see that our Gibbs Sampler isn’t as good. While it manages to converge and fluctuate around the true \\(a\\) value, it hasn’t begun fluctuating around the true \\(b\\) value, even after 1000 trials.\r\nMain Takeaways\r\nGibbs Sampling is a great tool for simulating distributions of multivariate models when the joint distribution is too difficult to work with, but it is easy to sample from conditional probabilities.\r\nA well-performed Gibbs Sampler will have a high mixing rate and a low burn-in period.\r\nWhile it is not vital to choose a starting point near the true value, it is important that you have priors that reasonably match the data that you are trying to model. Otherwise, a Gibbs Sampling simulation may not find the correct values - remember that the priors are selected subjectively so it is important to carry more background research and choose wisely.\r\n\r\n\r\n\r\n",
      "last_modified": "2023-04-29T16:00:03-05:00"
    },
    {
      "path": "index.html",
      "title": "Mathematical Statistics: Gibbs Sampling",
      "description": "Welcome to the website. This website introduces Gibbs Sampling in Bayesian statistics world. This is a class project from course MATH/ STAT 455: Mathematical Statistics at Macalester College. The contents in this blog are collaborative efforts from Kristy Ma and Regan Brodine.\n",
      "author": [],
      "contents": "\r\nHi!\r\nWelcome to our project page on Gibbs Sampling - an example of Markov chain Monte Carlo (MCMC) algorithms that uses conditional probabilities to iteratively draw posterior samples for estimating the probabiltiy distribution of unknown parameters. This also ties with a cool application in statistical genetics to infer genotypes in unknown population groups with Hardy Weinberg Equilibrium.\r\nTo start exploring, simply navigate to tabs on the upper right corner:\r\n“Home”: This page\r\n“Intro to Bayesian”: Bayesian philosophy; Bayes rules; Posterior Exercise\r\n“Gibbs Sampling”: Gibbs Sampling Background; Properties and Limitations; Gibbs Sampling and MCMC; Simulation Example of Gibbs Sampler\r\n“Applications”: Gibbs Sampling in statistical genetics\r\n“Wrap-up”: Outro!\r\nHave fun!\r\nIf you have any questions, please reach out to kristy20011001@gmail.com and rbrodine@macalester.edu. Thank you!\r\n\r\n\r\n\r\n",
      "last_modified": "2023-04-29T16:00:04-05:00"
    },
    {
      "path": "sim.html",
      "title": "Applying Gibbs Sampling",
      "description": "Gibbs Sampling can be used in population genetics research to build populations.",
      "author": [],
      "contents": "\r\nGibbs Sampling in statistical genetics\r\nOften times in population genetics research, it is useful to classify individuals in a sample into populations. While other areas of study may use factors like linguistic, cultural, or physical characters, it is not always easy to assign populations based on genotypes.\r\nPritchard et al. 2000 proposed a method to assign individuals to populations and simultaneously estimate allele frequencies: homozygous dominant, heterozygous, and homozygous recessive. This method can utilize various genetic markers as indication of alleles including SNPS, RFLPS, micro-satellites, etc. Additionally, they follow a few assumptions:\r\nAssumes markers are unlinked loci → can be drawn as independent samples\r\nAssumes Hardy Weinberg Equilibrium\r\nHardy Weinberg Equilibrium\r\n\\[p^2+2pq+q^2 = 1\\]\r\nWhere \\(p^2\\) is the homozygous dominant allele frequency, \\(2pq\\) is the heterozygous allele frequency, and \\(q^2\\) is the homozygous recessive allele frequency. By assuming Hardy Weinberg Equilibrium, we are stating that the genetic variation in a population will remain constant from one generation to the next.\r\nUnder these assumptions, each allele at each locus in each genotype is an independent draw from the appropriate frequency distribution.\r\nBuilding a Model\r\nAssume each population is modeled by a characteristic set of allele frequencies.\r\nLet X be the genotypes of sampled individuals (our data)\r\nLet Z be the unknown population of origin of individuals\r\nLet P be the unknown allele frequency in all populations\r\nInformation about P and Z is given by the posterior distribution:\r\n\\[\\begin{aligned}P(Z, P |X) &= \\frac{P(Z, P, X)}{P(X)} \\\\\r\n&\\propto P(P) P(Z) P(X|Z, P)\r\n\\end{aligned}\\]\r\nThis is a great opportunity to use Gibbs Sampling! It is not possible to sample from the posterior, \\(P(Z, P|X)\\), directly. We don’t know what \\(P(P)\\) or \\(P(Z)\\) are because they are our unknown variables. However, it is possible to use conditional sampling to build an approximate distribution: \\((Z_1,P_1), (Z_2, P_2), ..., (Z_n, P_n)\\).\r\nStart with randomly drawn initial value \\(Z_0\\) as a hypothetical population of origin and \\(P_0\\) as a hypothetical allele frequency. Then, iterate the following steps:\r\nSample \\(P_1\\) from \\(P(P | X, Z_0)\\)\r\nThis estimates allele frequencies for each population assuming population of origin for the individual is known.\r\n\r\nSample \\(Z_1\\) from \\(P(Z | X, P_1)\\)\r\nThis estimates the population of origin for each individual assuming that the population allele frequencies are known.\r\n\r\nContinue the pattern \\(n\\) times:\r\nSample \\(P_n\\) from \\(P(P | X, Z_{n-1})\\)\r\nSample \\(Z_n\\) from \\(P(Z | X, P_n)\\)\r\n\r\n",
      "last_modified": "2023-04-29T16:00:05-05:00"
    },
    {
      "path": "wrapup.html",
      "title": "Outro",
      "description": "Thank you for reading!",
      "author": [],
      "contents": "\r\nThank you again for having interest on our page.\r\n\r\n\r\n\r\n",
      "last_modified": "2023-04-29T16:00:06-05:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
