{
  "articles": [
    {
      "path": "about.html",
      "title": "Bayesian statistical world",
      "description": "Some additional details about the website\n",
      "author": [],
      "contents": "\nBackground: Bayesian V.S. Frequentist\nBayes Rule\n\n\n\n",
      "last_modified": "2023-03-22T14:31:08-05:00"
    },
    {
      "path": "gibbs.html",
      "title": "Gibbs Sampling",
      "description": "Some additional details about the website",
      "author": [],
      "contents": "\nConcept\nGibbs Sampling is a Markov Chain Monte Carlo (MCMC) method that that is used for multidimensional models. Gibbs is used when sampling using joint distributions is too difficult, but it is easy to sample from conditional probabilities.\nGibbs Sampling Definition\nSuppose we want to obtain \\(n\\) samples of \\(X = (x_1, x_2, x_3, ..., x_m)\\) from a joint distribution \\(p(x,y)\\).\nIn Gibbs sampling, we will sample each \\(x^m\\) conditional on others, that is, in iteration \\((n+1)\\):\n$$\n\\[\\begin{aligned} x_{n+1}^{(1)}&\\sim P(x^{(1)}|x_n^{(2)},x_n^{(3)},...,x_n^{(m)}) \\\\\n\nx_{n+1}^{(2)}&\\sim P(x^{(2)}|x_{n+1}^{(1)},x_n^{(3)},...,x_n^{(m)})  \\\\\n\n\nx_{n+1}^{(m)}&\\sim P(x^{(m)}|x_{n+1}^{(1)},...,x_{n+1}^{(m-1)})\n\n\\end{aligned}\\]\n$$\nGibbs sampler is a special case of the Metropolis-Hasting algorithm with the conditional distribution (source: Jingyi Jessica Li)\nIn Practice\n1. Set \\((x_0,y_0)\\) to some starting value.\n2. Sample \\(x_1 \\sim p(x|y_0)\\). Alternatively, \\(X|Y = y_0\\). This produces \\((x_1, y_0)\\).\n3. Then, sample \\(y_1 \\sim p(y|x_1)\\) to arrive at the second point in the distribution \\((x_1,y_1)\\)\n4. Repeat steps 2 and 3, M times.\nThis produces a sequence of pairs of random variables: \\((X_0,Y_0),(X_1,Y_1),(X_2,Y_2),(X_3,Y_3),...\\) which satisfies the property of being a Markov chain.\nNote that the conditional distribution of \\((X_i,Y_i)\\), given all the previous pairs, only depends on \\((X_{i-1},Y_{i-1})\\) (Steorts)\nIn a Bayesian perspective, Gibbs Sampling can be used to simulate a Markov chain distribution of unknown parameter \\(\\theta\\), \\(\\pi (\\theta)\\).\nFor initial conditions, suppose \\(\\theta\\) may be partitioned into \\(\\theta = (\\theta_1,...,\\theta_r)\\). It is then possible to simulate a random value of \\(\\theta_i\\) from a full conditional distribution \\(\\pi(\\theta_i | \\theta_1, \\theta_2, ..., \\theta_{i-1},\\theta_{i+1},...,\\theta_r)\\) for \\(i = 1, 2, ...\\). Then, these initial conditions can be used to simulate the distribution \\(\\pi (\\theta)\\). Start with initial values \\(\\theta^{(0)} = (\\theta_1^{(0)},...,\\theta_r^{(0)})\\). Then, use the following steps for \\(m = 1, 2, ...\\):\nBayesian Notes\nStep 1: Sample \\(\\theta_1^{(m)}\\) from \\(\\pi(\\theta_1 | \\theta_2^{(m-1)}, \\theta_3^{(m-1)},..., \\theta_4^{(m-1)})\\)\nStep 2: Sample \\(\\theta_2^{(m)}\\) from \\(\\pi(\\theta_2 | \\theta_1^{(m)}, \\theta_3^{(m-1)},..., \\theta_d^{(m-1)})\\)\nStep r: Sample \\(\\theta_r^{(m)}\\) from \\(\\pi(\\theta_r | \\theta_1^{(m)}, \\theta_2^{(m)},..., \\theta_{r-1}^{(m)})\\)\nBy the Monte Carlo rules, it is easy to show that if \\(\\theta^{(m-1)}\\sim \\pi(\\theta)\\), then \\(\\theta^{(m)}\\sim \\pi(\\theta)\\), and thus is the distribution \\(\\pi (\\theta)\\). (Pritchard, 2000.)\nAn example\nConsider an exponential model for observations \\(x = (x_1,...,x_n)\\):\n\\[p(x|a,b) = abe^{-abx} \\quad \\text{for} (x>0)\\]\nand suppose the prior is:\nask kelsey about this\n\\[p(a,b) = e^{-a-b} \\quad \\text{for} (a,b>0)\\]\nWe want to sample from the posterior: \\(p(a,b|x)\\). To start, we must look at likelihood.\n\\[\\begin{aligned} p(\\vec{x}|a,b) &= \\Pi_{i=1}^n p(x_i|a,b) \\\\\n&= \\Pi_{i=1}^n abe^{-abx_i} \\\\\n&= (ab)^n e^{-ab\\sum_{i=1}^nx_i}\n\\end{aligned}\\]\nHowever, since the function is symmetric for a and b, we only need to solve for one. We can derive the conditional probability distributions, \\(A\\) and \\(B\\), respectively:\n\\[\\begin{aligned} p(a|x,b) &\\propto_a p(\\vec{x}|a,b)p(a,b) \\\\\n&= (ab)^n e^{-ab\\sum_{i=1}^nx_i} \\times e^{-a-b} \\\\\n&= (ab)^n e^{-a-b-ab\\sum_{i=1}^nx_i} \\\\\n&\\propto_a a^ne^{-a(b\\sum_{i=1}^nx_i)}\n\\end{aligned}\\]\nThus, the posterior distributions:\n\\[A \\sim \\text{Gamma}(n+1,b_n\\sum_{i=1}^nx_i +1)\\]\n\\[B \\sim \\text{Gamma}(n+1,a_n\\sum_{i=1}^nx_i +1)\\]\nThen, we can apply our posterior distribution to Gibbs Sampling to find an estimate of the rate, \\(\\lambda\\), for our distribution.\n\n\n\nWe are working with an exponential model that is dependent on variables a and b. Lambda = ab\nWe will write out the actual equation later.\n\n\nsampleGibbs <- function(start.a, start.b, n.sims, data){\n# get sum, which is sufficient statistic\nx <- sum(data)\n# get n\nn <- nrow(data)\n# create empty matrix, allocate memory for efficiency\nres <- matrix(NA, nrow = n.sims, ncol = 2)\nres[1,] <- c(start.a,start.b)\nfor (i in 2:n.sims){\n# sample the values\nres[i,1] <- rgamma(1, shape = n+1,\nrate = res[i-1,2]*x+1)\nres[i,2] <- rgamma(1, shape = n+1,\nrate = res[i,1]*x+1)\n}\nreturn(res) #this gives us a matrix of simulated drawings for a and b\n}\n\n\n\n\n# run Gibbs sampler\nn.sims <- 1000\na = 0.2\nb = 0.3 #these are our actual parameter values\n\nset.seed(455)\ndist <- rexp(n = n.sims, a*b)\ndata <-data.frame(dist)\n\n# return the result (res)\nres <- sampleGibbs(.25,.25, n.sims, data)\nres.mat <- data.frame(res)\n\nggplot(res.mat, aes(x = X1*X2))+ #we know from our exponential distribution that lambda = ab, so we can find the distribution of lambda for parameter estimation\n  geom_histogram()+\n  ggtitle(\"Distribution of Lambda\")\n\n\n\nLet’s take a look at a simple example with discrete random variables.\n# Gibbs Sampling and Markov chain Monte Carlo (MCMC)\n\n\n\n",
      "last_modified": "2023-03-22T14:31:10-05:00"
    },
    {
      "path": "index.html",
      "title": "Mathematical Statistics: Gibbs Sampling",
      "description": "Welcome to the website. This website introduces Gibbs Sampling in Bayesian statistic world. This is a class project from course MATH/ STAT 455: Mathematical Statistics at Macalester College. The contents in this blog are collaborative efforts from Kristy Ma and Regan Brodine.\n",
      "author": [],
      "contents": "\nHi!\nTo start exploring, please simply navigate to tabs on the upper right corner. The table of contents is here:\n“Home”: This page\n“Intro to Bayesian”: Bayesian philosophy; Bayes rules\n“Gibbs Sampling”: Gibbs Sampling Concept; Examples; Gibbs Sampling and Markov chain Monte Carlo (MCMC)\n“Applications”: Gibbs Sampling in statistical genetics; A simulation study\n“Wrap-up”: Outro; Other MCMC algorithms\nHave fun!\n\nIf you have any questions, please reach out to kristy20011001@gmail.com and rbrodine@macalester.edu. Thank you!\n\n\n\n\n",
      "last_modified": "2023-03-22T14:31:11-05:00"
    },
    {
      "path": "sim.html",
      "title": "Applying Gibbs Sampling",
      "description": "Gibbs Sampling can be used in population genetics research to build populations.\n",
      "author": [],
      "contents": "\n\n\n\n\n\n\n\n\nMathematical Statistic: Gibbs Sampling\n\n\nHome\nIntro to Bayesian\nGibbs Sampling\nApplications\nWrap-up\n☰\n\n\n  \n    \n      \n        \n        \n        \n      \n      \n    \n    \n      \n  Home\n\n\n  Intro to Bayesian\n\n\n  Gibbs Sampling\n\n\n  Applications\n\n\n  Wrap-up\n\n      \n  \n\n\n\n\n\n\nApplying Gibbs Sampling\n\n\n\n\n\nGibbs Sampling in statistical genetics\nOften times in population genetics research, it is useful to classify\nindividuals in a sample into populations. While other areas of study may\nuse factors like linguistic, cultural, or physical characters, it is not\nalways easy to assign populations based on genotypes.\nPritchard et al. 2000 proposed a method to assign individuals to\npopulations and simultaneously estimate allele frequencies: homozygous\ndominant, heterozygous, and homozygous recessive. This method can\nutilize various genetic markerss as indication of alleles including\nSNPS, RFLPS, microsatellites, etc. Additionally, they follow a few\nassumptions:\nAssumes markers are unlinked loci → can be drawn as independent\nsamples\nAssumes Hardy Weinberg Equilibrium\nHardy Weinberg Equilibrium \\[p^2+2pq+q^2 = 1\\] Where \\(p^2\\) is the homozygous dominant allele\nfrequency, \\(2pq\\) is the heterozygous\nallele frequency, and \\(q^2\\) is the\nhomozygous recessive allele frequency. By assuming Hardy Weinberg\nEquilibrium, we are stating that the genetic variation in a population\nwill remain constant from one generation to the next.\nUnder these assumptions, each allele at each locus in each genotype\nis an independent draw from the appropriate frequency distribution.\n\n\nBuilding a Model\nAssume each population is modeled by a characteristic set of allele\nfrequencies.\nLet X be the genotypes of sampled individuals (our data)\nLet Z be the unknown population of origin of individuals\nLet P be the unknown allele frequency in all populations\nInformation about P and Z is given by the posterior distribution:\n\\[\\begin{aligned}P(Z, P |X) &=\n\\frac{P(Z, P, X)}{P(X)} \\\\\n&\\propto P(P) P(Z) P(X|Z, P)\n\\end{aligned}\\]\nThis is a great opportunity to use Gibbs Sampling! It is not possible\nto sample from the posterior, \\(P(Z,\nP|X)\\), directly. We don’t know what \\(P(P)\\) or \\(P(Z)\\) are because they are our unknown\nvariables. However, it is possible to use conditional sampling to build\nan approximate distribution: \\((Z_1,P_1),\n(Z_2, P_2), ..., (Z_n, P_n)\\).\nStart with randomly drawn initial value \\(Z_0\\) as a hypothetical population of\norigin and \\(P_0\\) as a hypothetical\nallele frequency. Then, iterate the following steps:\nSample \\(P_1\\) from \\(P(P | X, Z_0)\\)\nThis estimates allele frequencies for each population assuming\npopulation of origin for the individual is known.\n\nSample \\(Z_1\\) from \\(P(Z | X, P_1)\\)\nThis estimates the population of origin for each individual assuming\nthat the population allele frequencies are known.\n\nContinue the pattern \\(n\\) times:\nSample \\(P_n\\) from \\(P(P | X, Z_{n-1})\\)\nSample \\(Z_n\\) from \\(P(Z | X, P_n)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "last_modified": "2023-04-25T13:41:37-05:00"
    },
    {
      "path": "wrapup.html",
      "title": "Outro",
      "description": "Some additional details about the website",
      "author": [],
      "contents": "\nA few more…\n\n\n\n",
      "last_modified": "2023-03-22T14:31:12-05:00"
    }
  ],
  "collections": []
}
