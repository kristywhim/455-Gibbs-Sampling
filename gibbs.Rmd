---
title: "Gibbs Sampling"
description: |
  Some additional details about the website
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Concept {#gibbs}

Gibbs Sampling is a Markov Chain Monte Carlo (MCMC) method that that is used for multidimensional models. Gibbs is used when sampling using joint distributions is too difficult, but it is easy to sample from conditional probabilities.

**Gibbs Sampling Definition**
Suppose we want to obtain $n$ samples of $X = (x_1, x_2, x_3, ..., x_d)$ from a joint distribution $p(x,y)$. 

*Overview*
1. Set $(x_0,y_0)$ to some starting value.
2. Sample $x_1 \sim p(x|y_0)$. Alternatively, $X|Y = y_0$. This produces $(x_1, y_0)$.
3. Then, sample $y_1 \sim p(y|x_1)$ to arrive at the second point in the distribution $(x_1,y_1)$
4. Repeat steps 2 and 3, M times.

This produces a sequence of pairs of random variables: $(X_0,Y_0),(X_1,Y_1),(X_2,Y_2),(X_3,Y_3),...$ which satisfies the property of being a Markov chain.

Note that the conditional distribution of $(X_i,Y_i)$, given all the previous pairs, only depends on $(X_{i-1},Y_{i-1})$
(Steorts)


In a Bayesian perspective, Gibbs Sampling can be used to simulate a Markov chain distribution of unknown parameter $\theta$, $\pi (\theta)$. 

For initial conditions, suppose $\theta$ may be partitioned into $\theta = (\theta_1,...,\theta_r)$. It is then possible to simulate a random value of $\theta_i$ from a full conditional distribution $\pi(\theta_i | \theta_1, \theta_2, ..., \theta_{i-1},\theta_{i+1},...,\theta_r)$ for $i = 1, 2, ...$. Then, these initial conditions can be used to simulate the distribution $\pi (\theta)$. Start with initial values $\theta^{(0)} = (\theta_1^{(0)},...,\theta_r^{(0)})$. Then, use the following steps for $m = 1, 2, ...$:

Step 1: Sample $\theta_1^{(m)}$ from $\pi(\theta_1 | \theta_2^{(m-1)}, \theta_3^{(m-1)},..., \theta_4^{(m-1)})$

Step 2: Sample $\theta_2^{(m)}$ from $\pi(\theta_2 | \theta_1^{(m)}, \theta_3^{(m-1)},..., \theta_d^{(m-1)})$

Step r: Sample $\theta_r^{(m)}$ from $\pi(\theta_r | \theta_1^{(m)}, \theta_2^{(m)},..., \theta_{r-1}^{(m)})$

By the Monte Carlo rules, it is easy to show that if $\theta^{(m-1)}\sim \pi(\theta)$, then $\theta^{(m)}\sim \pi(\theta)$, and thus is the distribution $\pi (\theta)$. (Pritchard, 2000.)

# An example

Let's take a look at a simple example with discrete random variables.
# Gibbs Sampling and Markov chain Monte Carlo (MCMC)